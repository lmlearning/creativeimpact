import argparse
import json
import csv
import random
import os
from sentence_transformers import SentenceTransformer, util
import numpy as np # For converting tensor to float

# Initialize the model globally or pass it around to avoid reloading it multiple times.
# For simplicity in this script, we might load it in main if only used there,
# or globally if the score_socialchem function is called from multiple places independently.
# Let's try loading it once in main for now. A global instance would be:
# MODEL = SentenceTransformer('all-MiniLM-L6-v2')

def score_socialchem(generated_rot: str, gold_rot: str, model: SentenceTransformer) -> float:
    """
    Scores the generated Rule of Thumb (ROT) against a gold ROT using sentence embeddings.
    Args:
        generated_rot: The ROT generated by the language model.
        gold_rot: The ground truth ROT.
        model: The pre-loaded SentenceTransformer model.
    Returns:
        The cosine similarity score between the two ROTs.
    """
    if not generated_rot or not gold_rot:
        return 0.0 # Or handle as an error, depending on desired behavior for empty strings

    # Encode both ROTs
    embedding_generated = model.encode(generated_rot, convert_to_tensor=True)
    embedding_gold = model.encode(gold_rot, convert_to_tensor=True)

    # Compute cosine-similarity
    cosine_scores = util.cos_sim(embedding_generated, embedding_gold)

    # Convert tensor to float. cos_sim returns a tensor of shape (1,1)
    score = cosine_scores.item() if np.prod(cosine_scores.shape) == 1 else cosine_scores[0][0].item()
    return score

def main():
    parser = argparse.ArgumentParser(description="Evaluate SocialChem outputs and generate a CSV.")
    parser.add_argument(
        "--input_file",
        required=True,
        help="Path to the JSON input file generated by generate_outputs_socialchem.py."
    )
    parser.add_argument(
        "--output_file",
        required=True,
        help="Full path for the output CSV file (e.g., outputs/scores/scores_socialchem.csv)."
    )
    args = parser.parse_args()

    output_dir = os.path.dirname(args.output_file)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    try:
        with open(args.input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"Error: Input file {args.input_file} not found.")
        return
    except json.JSONDecodeError:
        print(f"Error: Could not decode JSON from {args.input_file}.")
        return

    # Load the sentence transformer model
    print("Loading sentence transformer model 'all-MiniLM-L6-v2'...")
    model = SentenceTransformer('all-MiniLM-L6-v2')
    print("Model loaded.")

    csv_header = ["item_id", "plain_similarity_score", "creative_similarity_score", "plain_pass", "creative_pass"]

    with open(args.output_file, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(csv_header)

        for item in data:
            item_id = item.get("item_id", "unknown_id")
            if item.get("error"):
                print(f"Skipping item {item_id} due to generation error: {item['error']}")
                writer.writerow([item_id, "ERROR", "ERROR", "ERROR", "ERROR"])
                continue

            # These keys are assumed based on typical generation script outputs.
            # They might need adjustment if the actual JSON structure is different.
            gold_rot = item.get("ground_truth_rot", "") # Assuming this key holds the gold standard ROT
            plain_response_rot = item.get("plain_response", "") # Assuming this key holds the plain response
            creative_response_rot = item.get("creative_response", "") # Assuming this key holds the creative response

            if not gold_rot:
                print(f"Warning: Missing gold_rot for item {item_id}. Skipping scoring for this item.")
                writer.writerow([item_id, "NO_GOLD", "NO_GOLD", "NO_GOLD", "NO_GOLD"])
                continue

            plain_similarity_score = score_socialchem(plain_response_rot, gold_rot, model)
            creative_similarity_score = score_socialchem(creative_response_rot, gold_rot, model)

            # Convert similarity score to pass/fail using a 0.5 threshold
            # This threshold is an assumption for the subtask.
            plain_pass = 1 if plain_similarity_score >= 0.5 else 0
            creative_pass = 1 if creative_similarity_score >= 0.5 else 0

            writer.writerow([
                item_id,
                f"{plain_similarity_score:.4f}",
                f"{creative_similarity_score:.4f}",
                plain_pass,
                creative_pass
            ])

    print(f"Successfully generated SocialChem scores to {args.output_file}")

def run_simple_test():
    print("\n--- Running simple test for score_socialchem ---")
    # It's better to load the model once if testing multiple times or pass it
    model = SentenceTransformer('all-MiniLM-L6-v2')

    rot1 = "It's good to be kind to others."
    rot2 = "Being kind to people is a positive trait."
    rot3 = "An apple is a type of fruit."

    score12 = score_socialchem(rot1, rot2, model)
    score13 = score_socialchem(rot1, rot3, model)
    score23 = score_socialchem(rot2, rot3, model)
    score_empty1 = score_socialchem("", rot1, model)
    score_empty2 = score_socialchem(rot1, "", model)


    print(f"Similarity between '{rot1}' and '{rot2}': {score12:.4f}")
    print(f"Similarity between '{rot1}' and '{rot3}': {score13:.4f}")
    print(f"Similarity between '{rot2}' and '{rot3}': {score23:.4f}")
    print(f"Similarity between '' and '{rot1}': {score_empty1:.4f}")
    print(f"Similarity between '{rot1}' and '': {score_empty2:.4f}")
    print("--- Simple test finished ---")

if __name__ == '__main__':
    # When invoked as a script, run the main logic.
    # run_simple_test() is useful for direct testing of functions.
    main()
    # To run local tests for the functions:
    # print("--- Running local tests for socialchem functions (call run_simple_test() directly if needed) ---")
    # run_simple_test()
